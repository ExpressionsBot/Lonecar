### **Framework for Graph Prompt Learning**

---

#### **1. Core Challenges in Graph Prompting**

**1.1 Cross-Modality Problem**:

- **Definition**: Graphs need to work with other data formats (text, image) efficiently.
- **Example**: A knowledge graph might need to incorporate textual information or image data.
- **Solution**: Use pre-trained models that can handle both graph-structured data and linear data like text and images. This demands integrating cross-modality knowledge seamlessly into the graph structure.

**1.2 Cross-Domain Generalization**:

- **Definition**: Knowledge learned in one graph domain (e.g., social networks) must generalize to others (e.g., biology).
- **Example**: A model trained on a citation graph should be able to adapt to a drug interaction graph.
- **Solution**: Leverage domain-agnostic prompts that abstract out domain-specific features, allowing flexible transitions across domains.

**1.3 Cross-Task Adaptation**:

- **Definition**: The same model must be capable of adapting to different tasks, like node classification, edge prediction, and graph-level tasks.
- **Example**: Moving from node-level prediction (e.g., classifying users in a network) to graph-level classification (e.g., classifying types of molecular structures).
- **Solution**: Graph prompts should be task-agnostic at the pre-training stage, designed to enable downstream adaptation through minimal tuning.

---

#### **2. Graph Prompt Components**

**2.1 Prompt Tokens**:

- **Purpose**: Acts as the building blocks, representing extra information that can be added to graphs to guide the model’s attention.
    - **Token Types**:
        - **Task Tokens**: Represent specific task-related information (e.g., a node label for node classification).
        - **Structure Tokens**: Capture local or global graph structures (e.g., a subgraph for a node or the whole graph structure for classification).
    - **Learnability**: Tokens can either be **predefined** (static) or **learnable** (tunable), depending on the task and context.

**2.2 Token Structures**:

- **Purpose**: Define how tokens relate to each other and the original graph.
    - **Structure Types**:
        - **Independent Tokens**: Each token has no relation to the others.
        - **Interconnected Tokens**: Tokens are linked to capture higher-order interactions (e.g., creating a mini subgraph of tokens).
        - **Cross-Tokens**: Create a structure where prompt tokens interact with the original graph nodes, either directly or indirectly through edges.

**2.3 Insertion Patterns**:

- **Purpose**: Describes how prompt tokens are injected into the original graph structure.
    - **Insertion Strategies**:
        - **Cross-Linking**: Prompt tokens are connected to specific nodes in the original graph via new edges (e.g., a prompt for molecular bonds).
        - **Concatenation**: Tokens are concatenated with node features, influencing all downstream tasks.
        - **Multiplicative Updates**: Modify node embeddings via element-wise multiplication with prompt vectors, adjusting graph embeddings subtly.

---

#### **3. Pre-Training Strategies for Graph Prompts**

**3.1 Node-Level Pre-Training**:

- **Objective**: Learn local structures and features of individual nodes to generalize across tasks.
    - **Techniques**:
        - **Contrastive Learning**: Maximize mutual information between node views generated by perturbations (e.g., removing edges).
        - **Predictive Models**: Use masked nodes or substructures for node recovery to build robust node embeddings.

**3.2 Edge-Level Pre-Training**:

- **Objective**: Enhance the model’s ability to capture node-to-node interactions, crucial for tasks like link prediction.
    - **Techniques**:
        - **Edge Discrimination**: Contrastive approaches between real and fake edges.
        - **Edge Masking**: Mask edges and require the model to reconstruct the graph connectivity.

**3.3 Graph-Level Pre-Training**:

- **Objective**: Capture holistic graph information that’s useful for graph classification tasks.
    - **Techniques**:
        - **Graph Reconstruction**: Predict masked portions of the graph.
        - **Graph Contrastive Learning**: Maximize the similarity between subgraphs and global graph structures.

---

#### **4. Task Reformulation via Prompting**

**4.1 Unifying Tasks via Graph Prompts**:

- **Objective**: Reformulate tasks such as node classification, edge prediction, and graph classification into the same pretext task format during training.
    - **Process**:
        - **Node-Level to Graph-Level**: Convert node classification into a graph-level task by treating the node and its surrounding subgraph as a mini-graph.
        - **Edge Prediction as Link Prediction**: Reformulate link prediction tasks as part of a general graph comparison problem.

**4.2 Answering Functions**:

- **Objective**: The function through which predictions are derived from the pre-trained prompt model.
    - **Types**:
        - **Learnable Answering Functions**: Trainable heads (e.g., MLPs) that interpret prompt-influenced graph embeddings.
        - **Hand-crafted Answering Functions**: Pre-defined logic that directly maps graph embeddings to answers (e.g., using cosine similarity between node embeddings to predict class).

---

#### **5. Graph Prompt Tuning Strategies**

**5.1 Meta-Learning for Prompt Generalization**:

- **Objective**: Learn a generalized prompt that can be adapted across multiple tasks using meta-learning frameworks.
    - **Example**: MAML-based methods where a prompt is fine-tuned on task-specific objectives after being pre-trained on broad tasks.

**5.2 Task-Specific Prompt Tuning**:

- **Objective**: Directly tune prompts on a task-by-task basis.
    - **Example**: Using node classification labels to tune specific task-related tokens.

**5.3 Pretext-Aligned Prompt Tuning**:

- **Objective**: Align prompt tuning with pre-training objectives to ensure the prompt mechanism generalizes better across similar tasks.
    - **Example**: If the pre-training task is contrastive node-pair learning, the downstream task will adopt a similar contrastive learning objective for fine-tuning.

---

#### **6. Deployment in Real-World Applications**

**6.1 ProG Library**:

- **Description**: ProG is a Python library that facilitates prompt learning with graphs, providing essential functions for prompt insertion, pre-training, and task alignment.

**6.2 Use Cases**:

- **Drug Discovery**: Graph prompts could aid in molecular graph analysis, aligning chemical structure classification with pre-trained AGI models.
- **Social Network Analysis**: Prompted models could identify fake accounts or predict link formations more efficiently than traditional models.

---

#### **7. Challenges and Open Questions**

**7.1 Scalability**:

- **Challenge**: How to efficiently scale prompts to very large graphs without significant computational overhead.

**7.2 Cross-Modality Integration**:

- **Challenge**: Graph prompts still struggle with data from linear domains (e.g., combining text, images, and graphs meaningfully in a single model).

**7.3 Few-shot/Zero-shot Learning**:

- **Challenge**: Can prompts be fine-tuned or generalized to few-shot or zero-shot tasks efficiently, reducing reliance on large labeled datasets?

---

### **Conclusion**

Graph prompt learning is a highly modular and adaptive technique designed to overcome major limitations in AGI’s application to graph-based tasks. Through the use of flexible prompt tokens, task reformulation, and robust pre-training strategies, this approach opens up new possibilities for integrating and scaling AGI across different domains, tasks, and modalities. As we move towards a more interconnected world, graph prompts offer a powerful solution for making sense of complex, non-linear data structures, and this framework serves as a comprehensive guide for its development and application.